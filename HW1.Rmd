---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE)
```

<div align="right">
**\* R 마크다운으로 작성하여 pdf로 변환한 과제입니다. \***

<div align="left">
#### 1. Consider the `stock.csv` data in the lecture. We will fit the linear regression model by using Stock as response variable and Interest, Unemployment, Month dummy variable as covariates.

```{r 1-1}
data <- read.csv('stock.csv')
head(data)
```

하위 문제를 풀기 전에 먼저 `stock.csv` 파일을 열어 `data`라는 변수에 할당했다.

```{r 1-2}
attach(data)
```

향후 편리성을 위해 변수명을 직접 불러올 수 있게 `attach()` 함수를 적용했다.

##### (a) (10 point) Construct the dummy variable matrix for Month variable. Except for January, make 11 dummy variables (Hint: dummy variable matrix is 24 $\times$ 11).


```{r 1a}
dummy <- model.matrix(~factor(Month))[, -1]
colnames(dummy) <- 2:12
dummy
```

`model.matrix()` 함수로 dummy variable matrix를 만들어준 뒤, 첫 번째 열을 제거해주기 위해 인덱싱을 했다. <br>
출력값을 확인해보니 열 이름이 너무 길어 가시성이 떨어졌기 때문에 열 이름도 모두 숫자로 바꿨다.

##### (b) (10 point) Construct the design matrix $X$ by using intercept, Interest, Unemployment, and Month dummy variable (Hint: $X$ is 24 $\times$ 14).


```{r 1b}
X <- cbind(1, Interest, Unemployment, dummy)
X
```

`cbind()`로 intercept 열에 해당하는 1로 이루어진 열, Interest, Unemployment, 그리고 1(a)에서 이미 만들어놓은 Month dummy variable matrix `dummy`까지 한 개의 design matrix로 묶었다.

##### (c) (15 point) Calculate $\hat\beta=(X'X)^{-1}X'y$ and standard error of $\hat\beta$ without using `lm` function in `R`.

```{r 1c-1}
y <- Stock
dim(X); length(y)
```

우선 Stock을 종속변수 y로 지정했다.<br>
혹시 몰라 X의 행 수와 y의 길이가 일치하는지도 확인했다.

```{r 1c-2}
beta.hat <- solve(t(X)%*%X)%*%t(X)%*%y
beta.hat
```

$(X'X)^{-1}X'y$ 식의 $X$와 $y$ 자리에 X, y를 대입해 $\hat\beta$을 계산했다. <br>
$\hat\beta$은 standard error를 구할 때도 사용되므로 `beta.hat`이라는 변수에 저장했다.<br> <br>

Standard error를 구하기 위해서는 $\hat\sigma^2$를 먼저 구해야한다. <br>
$\hat\sigma^2$는 $\frac{(y-X\hat\beta)'(y-X\hat\beta)}{n-p-1}$로 구할 수 있다. <br>
이 경우 $n$은 24, $p$는 13이다.

```{r 1c-3}
sigmasq.hat <- as.numeric(t(y-X%*%beta.hat)%*%(y-X%*%beta.hat)/(24-13-1))
sigmasq.hat
```

$\frac{(y-X\hat\beta)'(y-X\hat\beta)}{n-p-1}$ 식에 $\hat\beta$과 $X$, $y$, $n$, $p$를 대입해 구한 $\hat\sigma^2$를 `sigmasq.hat`라는 변수에 저장했다. <br>
이때 `as.numeric`을 해주지 않으면 1 $\times$ 1 dataframe 꼴이 출력된다.

``` {r 1c-4}
var_cov_mat <- solve(t(X)%*%X)*sigmasq.hat
var_cov_mat
```

$(X'X)^{-1}\hat\sigma^2$를 통해 구한 $\hat\beta$의 variance covariance matrix다. <br>
이제 이 matrix의 대각성분에 `sqrt()`를 씌워주면 구하고자하는 $\hat\beta$의 standard error 값들이 나올 것이다.

``` {r 1c-5}
sqrt(diag(var_cov_mat))
```

##### (d) (15 point) Now using `lm` function, fit the same linear regression model. Then compare the results with $\hat\beta$ and standard errors calculated in problem 1(c).

```{r 1d}
model <- lm(y~X)
model$coefficients; summary(model)
```

`lm()` 함수에 독립변수, 종속변수를 X와 y로 지정해 linear regression model을 `model` 변수에 저장했다. <br>
`model$coefficients`는 `model`의 $\hat\beta$들을 출력해준다. 1(c)에서 구한 값들과 일치하는 것을 알 수 있다. <br>
Standard error는 `summary(model)`의 `Std. Error` 열에 출력된다. 역시 1(c)에서 구한 값들과 일치한다. <br> <br>
함수 하나로 1초만에 구해지는데 식 쓰느라 고생한 게 좀 현타온다.. 교수님이 분명 식으로 구하는 건 몰라도 된다고 그냥 교육적인 목적에서 한번 훑고 지나가겠다 하셨는데.... 교수님 넘행......

#### 2. Consider the `BrainTumor.csv` data set uploaded at LearnUs. We will fit the logistic regression model by using Class (1 = Tumor, 0 = Non-Tumor) as a response variable and 12 feature variables (Mean, Variance, Standard Deviation, Entropy, Skewness, Kurtosis, Contrast, Energy, ASM, Homogeneity, Dissimilarity, Correlation) as covariates. Note that these feature variables are obtained from MRI images.

```{r 2-1}
data2 <- read.csv('BrainTumor.csv')
head(data2)
```

하위 문제를 풀기 전에 먼저 `BrainTumor.csv` 파일을 열어 `data2`라는 변수에 할당했다.

```{r 2-2}
attach(data2)
```

향후 편리성을 위해 변수명을 직접 불러올 수 있게 `attach()` 함수를 적용했다.


##### (a) (15 point) Scale 12 feature variables (use `scale(x, center=TRUE, scale=TRUE)` function in R). Then draw a scatter plot for the scaled variables. Explain the relationship between variables from the scatter plot (point out at least three findings).

```{r 2a-1}
for (colname in colnames(data2)[-1]) {
  data2[colname] <- scale(data2[colname], center=TRUE, scale=TRUE)}
head(data2)
```

for문을 이용해 1열을 제외한 모든 열에 scaling을 적용했다.

```{r 2a-2}
pairs(data2[-1])
```

`pairs()` 함수를 적용해 변수 간 관계를 나타내는 scatter plot을 그려봤다. <br>
1) 우선 가장 눈에 띄는 것은 가장 선명한 상관관계를 보이고 있는 `Variance`와 `Standard Deviation`이다. 분산은 표준편차의 제곱이고, 표준편차는 분산의 제곱근이니 당연한 결과다. 상관관계 그래프가 $y=\sqrt{x}$와 $y=x^2$의 형태를 띠고 있다. <br>
2) 반대로 강한 반비례 관계를 나타내고 있는 변수는 `Variance`와 `Kurtosis`, `Standard Deviation`과 `Kurtosis`다. Kurtosis는 '첨도'라는 뜻으로, 분산이 작으면 첨도가 크고, 분산이 크면 첨도가 작을 수밖에 없다. <br>
3) 정확히 어떤 걸 나타내는 지표인지는 모르겠지만, `ASM`과 `Energy` 역시 강한 양의 상관관계를 보이고 있다. 구글링해보니 역시나 두 지표 모두 MRI 사진에서 명암도의 균일함을 측정하는 지표라고 한다. <br>
이외에도 선형관계에 가까운 상관관계를 가지는 변수들이 존재하므로, 이 데이터로 prediction을 하게 된다면 어느정도 feature selection을 한 후에 진행하는 게 좋아보인다.

##### (b) (15 point) Divide the data into training (70%), and test (30%) sets. Fit the logistic regression using training set (you may use `glm` function in `R`). Which variables are significant? Provide interpretation of regression coefficients $\beta$ for all significant covariates.

```{r 2b-1}
set.seed(59)
size <- dim(data2)[1]
train_idx <- sample(1:size, size=0.7*size, replace=FALSE)
train_data <- data2[train_idx, ]
test_data <- data2[-train_idx, ]
```

`data2`의 행 수, 즉 index 수를 `size`로 지정하고, <br>
`sample()` 함수를 이용해 전체 index의 70%를 무작위로 비복원추출했다. <br>
그리고 이 index에 위치한 값들을 `train_data`로 배정했다. <br>
sampling된 70%의 index를 제외한 index에 위치한 값들은 `test_data`로 배정했다. <br>
random seed는 59로 설정했다.

```{r 2b-2}
dim(train_data); dim(test_data)
```

train test split이 잘 되었는지 확인하기 위해 각 데이터의 dimension을 확인해봤다.

```{r 2b-3}
fit <- glm(Class~Mean+Variance+Standard.Deviation+Entropy+Skewness+Kurtosis+Contrast+Energy+ASM+Homogeneity+Dissimilarity+Correlation, family="binomial", data=train_data)
summary(fit)
```

`glm()` 함수에 `family="binomial"` 하이퍼파라미터를 지정해주고 `fit` 변수에 train data의 logistic regression 결과를 저장했다. <br>
그리고 regression 결과를 보기 위해 `summary()` 함수를 출력했다. <br>
유의수준 0.001 수준 하에서 <br>
`Variance`, `Kurtosis`, `Homogeneity`, `Dissimilarity`는 `Class`와 유의미한 **음의 상관 관계**, <br>
`Standard Deviation`, `Skewness`, `Contrast`, `Correlation`은 `Class`와 **양의 상관 관계**를 나타내고 있다.<br>
유의미한 상관 관계를 나타내는 covariate들은 각 covariate 값이 1씩 증가할 때 종속변수의 확률값이 estimate된 coefficient 값만큼 변화한다.

##### (c) (20 point) Using the fitted model above, predict a response (Class) for test data sets. For predicted value, assign 1 class (Tumor) if predicted mean probability is greater than or equal to 0.5 (otherwise, assign 0 class (Non-Tumor)). Calculate the prediction accuracy for the test data set.

```{r 2c-1}
test_data[, 1] <- predict(fit, newdata=test_data[, -1], type="response")
```

모델 `fit`을 test data의 1열(`Class`)을 제외한 변수들을 바탕으로 하는 `predict()` 함수에 넣어 response variable을 predict했다. <br>
하이퍼파라미터로는 `type="response"`를 지정했다. <br>
predict한 확률값들은 test data의 1열 `Class`에 넣어주었다. <br>
이제 이 값들을 다시 0.5 이상이면 1로, 미만이면 0으로 교체해줄 것이다.

```{r 2c-2}
for (idx in (1:dim(test_data)[1])) {
  if (test_data[idx, 1] >= 0.5) {
    test_data[idx, 1] <- 1
  } else {
    test_data[idx, 1] <- 0}
}
head(test_data)
```

for문을 사용해 test data의 index를 차례차례 내려가며 첫 번째 column이 0.5 이상인 경우 1로, 그렇지 않은 경우 0으로 교체했다. <br>
교체가 잘 됐는지 보기 위해 test data의 데이터 6개만 조회해 봤다.

``` {r 2c-3}
head(data2[-train_idx, 1] == test_data[, 1], 100)
```

기존 데이터 test index의 Class 값과 predict한 test data의 Class 값이 얼마나 일치하는지 100개만 비교결과를 뽑아봤다. <br>
잘 예측한 것 같다. 대충 99%의 accuracy가 나올 것 같다.

``` {r 2c-4}
sum((data2[-train_idx, 1] == test_data[, 1]) / dim(test_data)[1])
```

마지막으로 $Accuracy = \frac{Correct Predictions}{Total Predictions}$를 계산했다. <br>
`TRUE` 값은 1로, `FALSE` 값은 0으로 계산되므로 `sum()` 함수를 적용하면 `TRUE` 값의 수를 구할 수 있다. <br>
최종 accuracy는 약 **98.67%**로, 100개만 뽑았을 때 예상한 정도와 비슷한 accuracy가 산출됐다.