{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare data: Vitamin D and Osteoporosis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. In our data(`healthTrain.csv`, `healthTest.csv`), there are 5 variables; Gender(A column; 1 if male, 0 for female), RIDAGEYR(B column), vitamin(C column), Calcium(D column), Osteop(E column). Our goal is to build a neural network for predicting binary Osteop variable(1 if osteoporosis; otherwise 0) using the other four variables(Gender, RIDAGEYR, vitamin, Calcium). You should solve the problem based on `HW4.ipynb` that I uploaded at LearnUs. In `HW4.ipynb`, I loaded the `healthTrain.csv` to fit our model later. Also, I added normalize layer to improve performance of the neural network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Search for the definitions of two functions:**\n",
    "- `tf.keras.layers.dense`\n",
    "- `tf.keras.layers.Dropout`\n",
    "\n",
    "**Explain (1) the role of these functions and (2) arguments(options) of these functions. You can take a look at descriptions from https://www.tensorflow.org/.\n",
    "But do not simply copy-paste the descriptions. You should paraphrase the descriptions. (25 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Role\n",
    "- `tf.keras.layers.Dense`: densely-connected(=fully connected) layer 추가\n",
    "- `tf.keras.layers.Dropout`: 일부 뉴런 누락시켜주는 layer 추가(overfitting 방지목적)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Arguments\n",
    "\n",
    "- `tf.keras.layers.dense` <br>\n",
    " *units*: 출력값의 dimension(출력될 node 개수) <br>\n",
    " *activation*: 사용할 activation function<br>\n",
    " *use_bias*: 해당 layer가 bias vector를 사용하는지 <br>\n",
    " *kernel_initializer*: weights matrix에 적용할 initializer <br>\n",
    " *bias_initializer*: bias vector에 적용할 initializer <br>\n",
    " *kernel_regularizer*: weights matrix에 적용할 regularizer <br>\n",
    " *bias_regularizer*: bias vector에 적용할 regularizer <br>\n",
    " *activity_regularizer*: activate된 출력값에 적용할 regularizer <br>\n",
    " *kernel_constraint*: weights matrix에 적용할 constraint <br>\n",
    " *bias_constraint*: bias vector에 적용할 constraint <br>\n",
    "<br>\n",
    "- `tf.keras.layers.Dropout` <br>\n",
    " *rate*: 뉴런을 drop시킬 비율 <br>\n",
    " *noise_shape*: 뉴런을 drop시킬 규칙(input 형식과 일치하게 입력해줘야 함) <br>\n",
    " *seed*: 랜덤값 seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) In HW4.ipynb, I wrote the skeleton of the neural network using tf.keras.models.Sequential. Feel free to use this to build your own neural network. You can choose the number of nodes, types of activation functions. But you have to use sigmoid activation function at the last layer for binary classifcation. You may add more hidden layers if you want. Using `model.summary` function, report the structure of your neural network. (25 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 4)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset_train = loadtxt('healthTrain.csv', delimiter=',')\n",
    "\n",
    "# Splitting the dataset into input(X) and output(y) variables\n",
    "x_train = dataset_train[:,0:4]\n",
    "y_train = dataset_train[:,4]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51271427 50.356857   60.863575    9.411029  ]\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(np.array(x_train))\n",
    "print(normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node 개수가 100개고 activation function으로는 relu를 사용하는 fully connected layer 2개와 0.4 rate로 뉴런을 drop 시켜주는 dropout layer 1개를 추가했다. <br>\n",
    "마지막 layer에서는 하나의 값이 출력되어 sigmoid function에 입력되어야 하기 때문에 출력 dimension을 1로 지정해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 4)                 9         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               500       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 10,710\n",
      "Trainable params: 10,701\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "완성된 모델은 normalization layer와 2개의 fully connected layer, 1개의 dropout layer, 그리고 최종 fully connected layer로 이루어진 sequential model이다. <br>\n",
    "Hidden layer 추가로 각 단계에서 $4\\times100+100$(bias) $=500$개, $100\\times100+100=10100$개, $100\\times1+1=101$개 parameter, 그리고 normalization에서 9개 non-trainable parameter까지 총 10710개 parameter가 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Compile your model through `model.compile` function. (1) Explain why we are using binary crossentropy loss function for this problem. (2) Fit the neural network using model.fit function. You may choose \"epochs\" by your self. (3)\n",
    "Report the accuracy for your model. (25 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Reason for crossentropy loss function <br>\n",
    "<br>\n",
    "Loss function은 출력값의 양식에 따라 사용해야할 종류가 달라진다. 특정 숫자가 출력되는 regression 데이터에서는 주로 MSE, RMSE 등이 사용되고, 0과 1처럼 레이블 값이 출력되는 classification 데이터에서는 주로 Cross Entropy가 사용된다. 위 데이터는 binary classification 데이터이므로, Cross Entropy 중에서도 Binary Cross Entropy를 loss function으로 사용했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Fitting the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9329\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9363\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1844 - accuracy: 0.9367\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1824 - accuracy: 0.9373\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1838 - accuracy: 0.9367\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9366\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.9367\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1835 - accuracy: 0.9370\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9367\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1820 - accuracy: 0.9377\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1816 - accuracy: 0.9373\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.9367\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9370\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.9366\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1822 - accuracy: 0.9371\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.9371\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9381\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1811 - accuracy: 0.9367\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1811 - accuracy: 0.9374\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1818 - accuracy: 0.9370\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9376\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9367\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1795 - accuracy: 0.9374\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1807 - accuracy: 0.9371\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1817 - accuracy: 0.9371\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9373\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1793 - accuracy: 0.9373\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.9376\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1790 - accuracy: 0.9370\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9370\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1795 - accuracy: 0.9370\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1805 - accuracy: 0.9374\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9379\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9377\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1795 - accuracy: 0.9379\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1801 - accuracy: 0.9373\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1789 - accuracy: 0.9370\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1786 - accuracy: 0.9380\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9369\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1786 - accuracy: 0.9373\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1775 - accuracy: 0.9376\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1801 - accuracy: 0.9371\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1787 - accuracy: 0.9380\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1802 - accuracy: 0.9369\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1781 - accuracy: 0.9369\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1784 - accuracy: 0.9374\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1795 - accuracy: 0.9367\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1779 - accuracy: 0.9371\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9376\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1780 - accuracy: 0.9371\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.9376\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1774 - accuracy: 0.9377\n",
      "Epoch 53/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.9373\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1785 - accuracy: 0.9377\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1785 - accuracy: 0.9366\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1773 - accuracy: 0.9367\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9383\n",
      "Epoch 58/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1759 - accuracy: 0.9379\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1783 - accuracy: 0.9377\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1775 - accuracy: 0.9379\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.9374\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1758 - accuracy: 0.9377\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1776 - accuracy: 0.9383\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.9371\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1778 - accuracy: 0.9374\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9381\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1777 - accuracy: 0.9373\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9381\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1775 - accuracy: 0.9376\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1762 - accuracy: 0.9380\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1762 - accuracy: 0.9380\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1748 - accuracy: 0.9389\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9380\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1761 - accuracy: 0.9384\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1758 - accuracy: 0.9383\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1763 - accuracy: 0.9376\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.9366\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9376\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9371\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9377\n",
      "Epoch 81/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1759 - accuracy: 0.9370\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.9386\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1762 - accuracy: 0.9380\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1752 - accuracy: 0.9374\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1757 - accuracy: 0.9374\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9373\n",
      "Epoch 87/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1757 - accuracy: 0.9373\n",
      "Epoch 88/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1751 - accuracy: 0.9376\n",
      "Epoch 89/100\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9373\n",
      "Epoch 90/100\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1745 - accuracy: 0.9371\n",
      "Epoch 91/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1749 - accuracy: 0.9380\n",
      "Epoch 92/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1750 - accuracy: 0.9370\n",
      "Epoch 93/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1756 - accuracy: 0.9376\n",
      "Epoch 94/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1737 - accuracy: 0.9380\n",
      "Epoch 95/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1746 - accuracy: 0.9381\n",
      "Epoch 96/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1742 - accuracy: 0.9376\n",
      "Epoch 97/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1757 - accuracy: 0.9387\n",
      "Epoch 98/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1747 - accuracy: 0.9384\n",
      "Epoch 99/100\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1748 - accuracy: 0.9384\n",
      "Epoch 100/100\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1742 - accuracy: 0.9376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12289803108>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 수를 100으로 지정해 fitting했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Accuracy <br>\n",
    "<br>\n",
    "Epoch=100일 때 train accuracy는 0.9376이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) In a similar way, load the test data set from `healthTest.csv`. Report the accuracy of your model using model.evaluate function. (25 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 - 0s - loss: 0.1700 - accuracy: 0.9432\n",
      "\n",
      "Accuracy: 0.9432300329208374\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "dataset_test = loadtxt('healthTest.csv', delimiter=',')\n",
    "\n",
    "# Test X, y split\n",
    "x_test = dataset_test[:,0:4]\n",
    "y_test = dataset_test[:,4]\n",
    "\n",
    "# Accuracy calculation\n",
    "test_loss, test_acc = model.evaluate(x_test,y_test, verbose=2)\n",
    "print('\\nAccuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`healthTest.csv`를 불러와 accuracy를 계산했다. 모델은 약 0.943의 accuracy를 가진다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
